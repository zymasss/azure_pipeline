# Azure Pipeline

L‚Äôobjectif √©tait de construire une pipeline compl√®te partant d‚Äôune base OLTP dans PostgreSQL, puis d‚Äôorchestrer son ingestion vers un Data Lake (ADLS2) via Azure Data Factory, de la transformer dans Azure Synapse Analytics, et enfin de la visualiser dans Power BI.
Avant d‚Äôentrer dans le pipeline, j‚Äôai d√ª cr√©er et configurer plusieurs services Azure.

Azure Data Factory üîÑ PostgreSQL ‚û°Ô∏è ADLS Gen2 ‚û°Ô∏è Azure Synapse Analytics ‚û°Ô∏è Power BI

### J'ai utilis√© le service Azure Database for PostgreSQL ‚Äì Flexible Server, donc cr√©√© un serveur PostgreSQL manag√©. Pourquoi ? Pas besoin d‚Äôune VM : Azure g√®re tout
Pour cela, j‚Äôai autoris√© mon adresse IP √† acc√©der au serveur, puis install√© psql et configur√© les variables d‚Äôenvironnement afin de me connecter depuis mon terminal local.
Ainsi j‚Äôai pu cr√©er la base OLTP qui sert de point de d√©part au pipeline.

### Et pour accueillir cette nouvelle base, le service Azure Data Lake Storage Gen2 (ADLS2) √©tait appropri√© car il supporte les formats optimis√©s pour Synapse et Power BI somme Parquet. Il est simple √† g√©rer et peu co√ªteux.
Celui-ci m‚Äôa servi de :
zone d‚Äôatterrissage des donn√©es extraites depuis PostgreSQL (Bronze layer),
zone d‚Äôentr√©e pour Synapse.

### Une fois stock√©es, j‚Äôai utilis√© Azure Synapse Analytics (OLAP) pour transformer les donn√©es et les exploiter ensuite sous Power BI
Pour cela, il a fallu cr√©er un SQL Dedicated Pool, qui est une base SQL ind√©pendante avec ses propres param√®tres.
Plusieurs √©tapes ont √©t√© n√©cessaires : cr√©ation d‚Äôun login sur le serveur SQL du workspace (dans la base master) / cr√©ation du user correspondant dans le dedicated pool /
attribution des permissions sur ce pool.
J‚Äôai ensuite cr√©√© une table interne afin de stocker les donn√©es provenant d‚ÄôADLS, puis utilis√©es pour le reporting.

### Une simple connexion de l‚Äôendpoint du workspace, du login et du user dans Power BI suffit : nous pouvons alors visualiser les donn√©es comme souhait√© !

### Enfin, pour orchestrer tous ces services j‚Äôai utilis√© Azure Data Factory
ADF offre des connecteurs natifs pour PostgreSQL et ADLS, un syst√®me de scheduling avec triggers, et une int√©gration simple avec Synapse.
J‚Äôai configur√© trois connexions s√©curis√©es (Linked Services) permettant √† ADF de communiquer avec : PostgreSQL, ADLS2, Synapse Analytics.

J‚Äôai ensuite construit une pipeline contenant deux activit√©s Copy Data :
une premi√®re activit√© avec pour source : PostgreSQL manag√©, et pour destination : ADLS Gen2 (Parquet),
une seconde activit√© avec pour source : ADLS2, et pour destination : Synapse Analytics afin de charger les donn√©es dans le SQL Dedicated Pool.

La pipeline peut √™tre d√©clench√©e manuellement, via un trigger planifi√©, ou depuis Airflow gr√¢ce √† AzureDataFactoryRunPipelineOperator.

Ce projet m‚Äôa permis d‚Äôapprendre :
-comment manipuler un PostgreSQL manag√© sur Azure
-comment orchestrer une ingestion avec Data Factory
-comment utiliser un SQL Pool d√©di√© dans Synapse
-comment connecter tous ces services entre eux via les Linked Services
-comment concevoir une architecture cloud compl√®te
